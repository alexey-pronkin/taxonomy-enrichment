{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import wordpunct_tokenize\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_synset(file, all_synsets=None):\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler)\n",
    "    if all_synsets is None:\n",
    "        all_synsets = {}\n",
    "    for element in soup.findAll('synset'):\n",
    "        all_synsets[element.attrs['id']] = {'name': element.attrs['ruthes_name'], 'definition': element.attrs['definition']}\n",
    "    return all_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_senses(file):\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler)\n",
    "    all_senses = defaultdict(list)\n",
    "    for element in soup.findAll('sense'):\n",
    "        all_senses[element.attrs['synset_id']].append(element.attrs['name'])\n",
    "    return all_senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wordnet(file, synsets, senses=None, G=None, directed=False):\n",
    "    if G is None:\n",
    "        if directed:\n",
    "            G = nx.DiGraph()\n",
    "        else:\n",
    "            G = nx.Graph()\n",
    "    if directed and type(G) != nx.classes.digraph.DiGraph:\n",
    "        raise Exception('Graph is not directed')\n",
    "    if not directed and type(G) != nx.classes.digraph.Graph:\n",
    "        raise Exception('Graph should not be directed')\n",
    "    \n",
    "    print('Input graph: {} nodes, {} edges'.format(len(G.nodes), len(G.edges)))\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler)\n",
    "    for element in soup.findAll('relation'):\n",
    "        relation = element.attrs\n",
    "        parent_id = relation['parent_id']\n",
    "        child_id = relation['child_id']\n",
    "        if relation['name'] in ['hyponym', 'instance hyponym']:\n",
    "            if parent_id not in G.nodes:\n",
    "                G.add_node(parent_id, in_edges=[], out_edges=[])\n",
    "            if child_id not in G.nodes:\n",
    "                G.add_node(child_id, in_edges=[], out_edges=[])\n",
    "            parent = G.nodes[parent_id]\n",
    "            child = G.nodes[child_id]\n",
    "            G.add_edge(parent_id, child_id)\n",
    "            if senses is not None:\n",
    "                parent_txt = copy.deepcopy(senses[parent_id])\n",
    "                child_txt = copy.deepcopy(senses[child_id])\n",
    "            else:\n",
    "                parent_txt = [synsets[parent_id]['name']]\n",
    "                child_txt = [synsets[child_id]['name']]\n",
    "            new_attr = {parent_id: {'out_edges': parent['out_edges'] + [child_id], 'text': parent_txt, 'definition': synsets[parent_id]['definition']},\n",
    "                        child_id: {'in_edges': child['in_edges'] + [parent_id], 'text': child_txt, 'definition': synsets[child_id]['definition']}}\n",
    "            nx.set_node_attributes(G, new_attr)\n",
    "    print('Updated graph: {} nodes, {} edges'.format(len(G.nodes), len(G.edges)))\n",
    "    for syn in synsets:\n",
    "        if syn not in G.nodes:\n",
    "            G.add_node(syn)\n",
    "            txt = senses[syn]\n",
    "            defn = synsets[syn]['definition']\n",
    "            nx.set_node_attributes(G, {syn: {'out_edges': [], 'in_edges': [], 'text': txt, 'definition': defn}})\n",
    "    print('Graph with orphan nodes: {} nodes, {} edges'.format(len(G.nodes), len(G.edges)))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_senses_noun = parse_senses('ruwordnet/senses.N.xml')\n",
    "all_senses_verb = parse_senses('ruwordnet/senses.V.xml')\n",
    "all_synsets_noun = parse_synset('ruwordnet/synsets.N.xml')\n",
    "all_synsets_verb = parse_synset('ruwordnet/synsets.V.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 29295 nodes, 39110 edges\n",
      "Graph with orphan nodes: 29296 nodes, 39110 edges\n",
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 7408 nodes, 10317 edges\n",
      "Graph with orphan nodes: 7521 nodes, 10317 edges\n"
     ]
    }
   ],
   "source": [
    "# wordnet graphs - undirected\n",
    "G_full_noun = parse_wordnet('ruwordnet/synset_relations.N.xml', all_synsets_noun, all_senses_noun)\n",
    "G_full_verb = parse_wordnet('ruwordnet/synset_relations.V.xml', all_synsets_verb, all_senses_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 29295 nodes, 39110 edges\n",
      "Graph with orphan nodes: 29296 nodes, 39110 edges\n",
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 7408 nodes, 10317 edges\n",
      "Graph with orphan nodes: 7521 nodes, 10317 edges\n"
     ]
    }
   ],
   "source": [
    "# wordnet graphs - directed\n",
    "G_full_dir_noun = parse_wordnet('ruwordnet/synset_relations.N.xml', all_synsets_noun, all_senses_noun, directed=True)\n",
    "G_full_dir_verb = parse_wordnet('ruwordnet/synset_relations.V.xml', all_synsets_verb, all_senses_verb, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root nodes: 9 nouns, 172 verbs\n",
      "Leaf nodes: 19083 nouns, 4631 verbs\n"
     ]
    }
   ],
   "source": [
    "roots_noun = [v for v in G_full_noun.nodes if len(G_full_noun.nodes[v]['in_edges']) == 0]\n",
    "leaves_noun = [v for v in G_full_noun.nodes if len(G_full_noun.nodes[v]['out_edges']) == 0]\n",
    "roots_verb = [v for v in G_full_verb.nodes if len(G_full_verb.nodes[v]['in_edges']) == 0]\n",
    "leaves_verb = [v for v in G_full_verb.nodes if len(G_full_verb.nodes[v]['out_edges']) == 0]\n",
    "print('Root nodes: {} nouns, {} verbs'.format(len(roots_noun), len(roots_verb)))\n",
    "print('Leaf nodes: {} nouns, {} verbs'.format(len(leaves_noun), len(leaves_verb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(G, roots, node):\n",
    "    paths = []\n",
    "    for r in roots:\n",
    "        try:\n",
    "            paths.append(nx.shortest_path_length(G, r, node))\n",
    "        except:\n",
    "            pass\n",
    "    return max(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaf nodes of depth 5+: 14649 nouns, 2357 verbs\n"
     ]
    }
   ],
   "source": [
    "leaf_d5_noun = [v for v in leaves_noun if get_depth(G_full_dir_noun, roots_noun, v) >= 5]\n",
    "leaf_d5_verb = [v for v in leaves_verb if get_depth(G_full_dir_verb, roots_verb, v) >= 5]\n",
    "print('Leaf nodes of depth 5+: {} nouns, {} verbs'.format(len(leaf_d5_noun), len(leaf_d5_verb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each connected component in its line\n",
    "# G has to be undirected\n",
    "def to_text_component(G, out_file, nodes=None, single_word=False):\n",
    "    out = open(out_file, 'w')\n",
    "    out.write('SYNSET_ID\\tTEXT\\tPARENTS\\tPARENT_TEXTS\\n')\n",
    "    if nodes is None:\n",
    "        nodes = G.nodes\n",
    "    for n in nodes:\n",
    "        \n",
    "        if single_word:\n",
    "            good_def = [txt for txt in G.nodes[n]['text'] if ' ' not in txt]\n",
    "            if len(good_def) > 0:\n",
    "                text = ','.join(good_def)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            text = '; '.join(G.nodes[n]['text'])\n",
    "        \n",
    "        parents = copy.deepcopy(G.nodes[n]['in_edges'])\n",
    "        full_parents = []\n",
    "        for p in parents:\n",
    "            p_of_p = copy.deepcopy(G.nodes[p]['in_edges'])\n",
    "            full_parents.extend([p] + p_of_p)\n",
    "        if len(full_parents) == 0:\n",
    "            full_parents.append('')\n",
    "        full_parents = list(set(full_parents))\n",
    "        \n",
    "        #print(full_parents)\n",
    "        subG = nx.subgraph(G, full_parents)\n",
    "        for c in nx.connected_components(subG):\n",
    "            parent_idx = []\n",
    "            parent_txt = []\n",
    "            for n_c in c:\n",
    "                parent_txt.append('; '.join(G.nodes[n_c]['text']))\n",
    "                parent_idx.append(n_c)\n",
    "            #parent_txt = str(parent_txt).replace('\\'', '\\\"')\n",
    "            #parent_idx = str(parent_idx).replace('\\'', '\\\"')\n",
    "            out.write('%s\\t%s\\t%s\\t%s\\n' % (n, text, json.dumps(parent_idx), json.dumps(parent_txt, ensure_ascii=False)))\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_text_component(G_full_noun, 'tt_ruthes_leaf_depth5_nouns_components_semicolon2.tsv', nodes=leaf_d5_noun)\n",
    "to_text_component(G_full_verb, 'tt_ruthes_leaf_depth5_verbs_components_semicolon2.tsv', nodes=leaf_d5_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
