{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import wordpunct_tokenize\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_synset(file, all_synsets=None):\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler)\n",
    "    if all_synsets is None:\n",
    "        all_synsets = {}\n",
    "    for element in soup.findAll('synset'):\n",
    "        all_synsets[element.attrs['id']] = {'name': element.attrs['ruthes_name'], 'definition': element.attrs['definition']}\n",
    "    return all_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_senses(file):\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler)\n",
    "    all_senses = defaultdict(list)\n",
    "    for element in soup.findAll('sense'):\n",
    "        all_senses[element.attrs['synset_id']].append(element.attrs['name'])\n",
    "    return all_senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wordnet(file, synsets, senses=None, G=None, directed=False):\n",
    "    if G is None:\n",
    "        if directed:\n",
    "            G = nx.DiGraph()\n",
    "        else:\n",
    "            G = nx.Graph()\n",
    "    if directed and type(G) != nx.classes.digraph.DiGraph:\n",
    "        raise Exception('Graph is not directed')\n",
    "    if not directed and type(G) != nx.classes.digraph.Graph:\n",
    "        raise Exception('Graph should not be directed')\n",
    "    \n",
    "    print('Input graph: {} nodes, {} edges'.format(len(G.nodes), len(G.edges)))\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler)\n",
    "    for element in soup.findAll('relation'):\n",
    "        relation = element.attrs\n",
    "        parent_id = relation['parent_id']\n",
    "        child_id = relation['child_id']\n",
    "        if relation['name'] in ['hyponym', 'instance hyponym']:\n",
    "            if parent_id not in G.nodes:\n",
    "                G.add_node(parent_id, in_edges=[], out_edges=[])\n",
    "            if child_id not in G.nodes:\n",
    "                G.add_node(child_id, in_edges=[], out_edges=[])\n",
    "            parent = G.nodes[parent_id]\n",
    "            child = G.nodes[child_id]\n",
    "            G.add_edge(parent_id, child_id)\n",
    "            if senses is not None:\n",
    "                parent_txt = copy.deepcopy(senses[parent_id])\n",
    "                child_txt = copy.deepcopy(senses[child_id])\n",
    "            else:\n",
    "                parent_txt = [synsets[parent_id]['name']]\n",
    "                child_txt = [synsets[child_id]['name']]\n",
    "            new_attr = {parent_id: {'out_edges': parent['out_edges'] + [child_id], 'text': parent_txt, 'definition': synsets[parent_id]['definition']},\n",
    "                        child_id: {'in_edges': child['in_edges'] + [parent_id], 'text': child_txt, 'definition': synsets[child_id]['definition']}}\n",
    "            nx.set_node_attributes(G, new_attr)\n",
    "    print('Updated graph: {} nodes, {} edges'.format(len(G.nodes), len(G.edges)))\n",
    "    for syn in synsets:\n",
    "        if syn not in G.nodes:\n",
    "            G.add_node(syn)\n",
    "            txt = senses[syn]\n",
    "            defn = synsets[syn]['definition']\n",
    "            nx.set_node_attributes(G, {syn: {'out_edges': [], 'in_edges': [], 'text': txt, 'definition': defn}})\n",
    "    print('Graph with orphan nodes: {} nodes, {} edges'.format(len(G.nodes), len(G.edges)))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_senses_noun = parse_senses('ruwordnet/senses.N.xml')\n",
    "all_senses_verb = parse_senses('ruwordnet/senses.V.xml')\n",
    "all_synsets_noun = parse_synset('ruwordnet/synsets.N.xml')\n",
    "all_synsets_verb = parse_synset('ruwordnet/synsets.V.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 29295 nodes, 39110 edges\n",
      "Graph with orphan nodes: 29296 nodes, 39110 edges\n",
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 7408 nodes, 10317 edges\n",
      "Graph with orphan nodes: 7521 nodes, 10317 edges\n"
     ]
    }
   ],
   "source": [
    "# wordnet graphs - undirected\n",
    "G_full_noun = parse_wordnet('ruwordnet/synset_relations.N.xml', all_synsets_noun, all_senses_noun)\n",
    "G_full_verb = parse_wordnet('ruwordnet/synset_relations.V.xml', all_synsets_verb, all_senses_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 29295 nodes, 39110 edges\n",
      "Graph with orphan nodes: 29296 nodes, 39110 edges\n",
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 7408 nodes, 10317 edges\n",
      "Graph with orphan nodes: 7521 nodes, 10317 edges\n"
     ]
    }
   ],
   "source": [
    "# wordnet graphs - directed\n",
    "G_full_dir_noun = parse_wordnet('ruwordnet/synset_relations.N.xml', all_synsets_noun, all_senses_noun, directed=True)\n",
    "G_full_dir_verb = parse_wordnet('ruwordnet/synset_relations.V.xml', all_synsets_verb, all_senses_verb, directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My fitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/anton/.local/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/anton/.local/lib/python3.8/site-packages (from gensim) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/lib/python3/dist-packages (from gensim) (1.14.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107092 sha256=b2294634837d29fc94ea91bb6dc249a0d435b0d586842ece7b235df8ded52402\n",
      "  Stored in directory: /home/anton/.cache/pip/wheels/11/73/9a/f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-3.0.0\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 29296/29296 [00:00<00:00, 34793.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from node2vec.node2vec import Node2Vec\n",
    "EMBEDDING_FILENAME = './embeddings.emb'\n",
    "EMBEDDING_MODEL_FILENAME = './embeddings.model'\n",
    "\n",
    "node2vec = Node2Vec(G_full_dir_noun, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=20, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1430799 ,  0.18411247,  0.05122835,  0.16415606,  0.08352555,\n",
       "        0.08574808, -0.04463342,  0.0211307 , -0.0652193 , -0.10945312,\n",
       "        0.23427185,  0.29137558, -0.01080806,  0.07309928,  0.17517462,\n",
       "        0.10879009,  0.30468294, -0.11301188, -0.15565468, -0.15395485,\n",
       "       -0.17999066, -0.18457739,  0.20708725,  0.11832654,  0.32384712,\n",
       "       -0.14635502,  0.18172409, -0.05882047, -0.03693543, -0.15229411,\n",
       "       -0.23149192, -0.22982502,  0.01131298,  0.21117437, -0.03521312,\n",
       "        0.1346391 ,  0.25796372,  0.08345957,  0.1226878 ,  0.03634934,\n",
       "        0.20238869, -0.03589795, -0.15335676, -0.07340327, -0.19338843,\n",
       "       -0.40288144, -0.1126978 , -0.04729822, -0.07889044,  0.15825468,\n",
       "        0.01519091, -0.11495806, -0.0963335 , -0.15838763,  0.1203206 ,\n",
       "       -0.14725037, -0.00323901,  0.24607028, -0.02629884,  0.10035977,\n",
       "       -0.04297424,  0.19750354,  0.04493263,  0.01982145], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['102214-N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE optimization between Fasttext and Node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import dataset, dataloader\n",
    "\n",
    "config = {\"db_path\": \"../data/ruwordnet.db\",\n",
    "    \"ruwordnet_path\": \"../data/ruwordnet/\",}\n",
    "ruwordnet = RuWordnet(db_path=config[\"db_path\"], ruwordnet_path=config[\"ruwordnet_path\"], with_lemmas=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model(model_path)\n",
    "ft.get_sentence_vector((\" \".join(texts)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "# from collections import defaultdict\n",
    "# noun_synsets = defaultdict(list)\n",
    "# verb_synsets = defaultdict(list)\n",
    "# for sense_id, synset_id, text in ruwordnet.get_all_senses():\n",
    "#     if synset_id.endswith(\"N\"):\n",
    "#         noun_synsets[synset_id].append(text.lower())\n",
    "#     elif synset_id.endswith(\"V\"):\n",
    "#         verb_synsets[synset_id].append(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuWordNetDataset(data.Dataset):\n",
    "    def __init__(self, ruwordnet, valid=False):\n",
    "        if valid:\n",
    "            self.data\n",
    "        self.data = list(ruwordnet.get_all_senses())\n",
    "        \n",
    "        self.size = len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    def __len__():\n",
    "        return self.size\n",
    "train_dataset = RuWordNetDataset(ruwordnet=ruwordnet)\n",
    "train_dataloader = dataloader.DataLoader(dataset=train_dataset)\n",
    "valid_dataset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Collecting catalyst\n",
      "  Downloading catalyst-20.11-py2.py3-none-any.whl (489 kB)\n",
      "\u001b[K     |████████████████████████████████| 489 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tensorboardX in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (2.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.1.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (5.8.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.33.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (4.50.2)\n",
      "Requirement already satisfied, skipping upgrade: GitPython>=3.1.1 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (3.1.8)\n",
      "Requirement already satisfied, skipping upgrade: deprecation in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard>=1.14.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (3.1.2)\n",
      "Requirement already satisfied, skipping upgrade: plotly>=4.1.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (4.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16.4 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.22 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (0.25.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from catalyst) (5.3)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboardX->catalyst) (3.13.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboardX->catalyst) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: future in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from torch>=1.1.0->catalyst) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from ipython->catalyst) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from ipython->catalyst) (47.3.0.post20200626)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from ipython->catalyst) (1.0.15)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from ipython->catalyst) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from ipython->catalyst) (2.7.1)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from ipython->catalyst) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from ipython->catalyst) (4.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from ipython->catalyst) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from packaging->catalyst) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from GitPython>=3.1.1->catalyst) (4.0.5)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboard>=1.14.0->catalyst) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboard>=1.14.0->catalyst) (1.31.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboard>=1.14.0->catalyst) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboard>=1.14.0->catalyst) (1.22.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboard>=1.14.0->catalyst) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboard>=1.14.0->catalyst) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboard>=1.14.0->catalyst) (3.3.2)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboard>=1.14.0->catalyst) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from tensorboard>=1.14.0->catalyst) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from matplotlib->catalyst) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from matplotlib->catalyst) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from matplotlib->catalyst) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from plotly>=4.1.0->catalyst) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from pandas>=0.22->catalyst) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->catalyst) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from traitlets>=4.2->ipython->catalyst) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->catalyst) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.1->catalyst) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\" in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (3.6.3)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=1.14.0->catalyst) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14.0->catalyst) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst) (3.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: multidict<5.0,>=4.5 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (4.7.5)\n",
      "Requirement already satisfied, skipping upgrade: yarl<1.6.0,>=1.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (20.2.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14.0->catalyst) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4; python_version < \"3.8\" in /home/g/miniconda3/envs/bml/lib/python3.7/site-packages (from yarl<1.6.0,>=1.0->aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (3.7.4.3)\n",
      "Installing collected packages: catalyst\n",
      "  Attempting uninstall: catalyst\n",
      "    Found existing installation: catalyst 20.9\n",
      "    Uninstalling catalyst-20.9:\n",
      "      Successfully uninstalled catalyst-20.9\n",
      "Successfully installed catalyst-20.11\n"
     ]
    }
   ],
   "source": [
    "!pip install -U catalyst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "model = torch.nn.Linear(28 * 28, 10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32),\n",
    "    \"valid\": DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32),\n",
    "}\n",
    "\n",
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def predict_batch(self, batch):\n",
    "        # model inference step\n",
    "        return self.model(batch[0].to(self.device).view(batch[0].size(0), -1))\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        # model train/valid step\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x.view(x.size(0), -1))\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        accuracy01, accuracy03 = metrics.accuracy(y_hat, y, topk=(1, 3))\n",
    "        self.batch_metrics.update(\n",
    "            {\"loss\": loss, \"accuracy01\": accuracy01, \"accuracy03\": accuracy03}\n",
    "        )\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "runner = CustomRunner()\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs\",\n",
    "    num_epochs=5,\n",
    "    verbose=True,\n",
    "    load_best_on_end=True,\n",
    ")\n",
    "# model inference\n",
    "for prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n",
    "    assert prediction.detach().cpu().numpy().shape[-1] == 10\n",
    "# model tracing\n",
    "traced_model = runner.trace(loader=loaders[\"valid\"])\n",
    "# data\n",
    "num_samples, num_features = int(1e4), int(1e1)\n",
    "X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n",
    "\n",
    "# model training\n",
    "runner = SupervisedRunner()\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logdir\",\n",
    "    num_epochs=8,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    synset_id, text = \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can get word by id from the database and do whatever we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root nodes: 9 nouns, 172 verbs\n",
      "Leaf nodes: 19083 nouns, 4631 verbs\n"
     ]
    }
   ],
   "source": [
    "roots_noun = [v for v in G_full_noun.nodes if len(G_full_noun.nodes[v]['in_edges']) == 0]\n",
    "leaves_noun = [v for v in G_full_noun.nodes if len(G_full_noun.nodes[v]['out_edges']) == 0]\n",
    "roots_verb = [v for v in G_full_verb.nodes if len(G_full_verb.nodes[v]['in_edges']) == 0]\n",
    "leaves_verb = [v for v in G_full_verb.nodes if len(G_full_verb.nodes[v]['out_edges']) == 0]\n",
    "print('Root nodes: {} nouns, {} verbs'.format(len(roots_noun), len(roots_verb)))\n",
    "print('Leaf nodes: {} nouns, {} verbs'.format(len(leaves_noun), len(leaves_verb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(G, roots, node):\n",
    "    paths = []\n",
    "    for r in roots:\n",
    "        try:\n",
    "            paths.append(nx.shortest_path_length(G, r, node))\n",
    "        except:\n",
    "            pass\n",
    "    return max(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaf nodes of depth 5+: 14649 nouns, 2357 verbs\n"
     ]
    }
   ],
   "source": [
    "leaf_d5_noun = [v for v in leaves_noun if get_depth(G_full_dir_noun, roots_noun, v) >= 5]\n",
    "leaf_d5_verb = [v for v in leaves_verb if get_depth(G_full_dir_verb, roots_verb, v) >= 5]\n",
    "print('Leaf nodes of depth 5+: {} nouns, {} verbs'.format(len(leaf_d5_noun), len(leaf_d5_verb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each connected component in its line\n",
    "# G has to be undirected\n",
    "def to_text_component(G, out_file, nodes=None, single_word=False):\n",
    "    out = open(out_file, 'w')\n",
    "    out.write('SYNSET_ID\\tTEXT\\tPARENTS\\tPARENT_TEXTS\\n')\n",
    "    if nodes is None:\n",
    "        nodes = G.nodes\n",
    "    for n in nodes:\n",
    "        \n",
    "        if single_word:\n",
    "            good_def = [txt for txt in G.nodes[n]['text'] if ' ' not in txt]\n",
    "            if len(good_def) > 0:\n",
    "                text = ','.join(good_def)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            text = '; '.join(G.nodes[n]['text'])\n",
    "        \n",
    "        parents = copy.deepcopy(G.nodes[n]['in_edges'])\n",
    "        full_parents = []\n",
    "        for p in parents:\n",
    "            p_of_p = copy.deepcopy(G.nodes[p]['in_edges'])\n",
    "            full_parents.extend([p] + p_of_p)\n",
    "        if len(full_parents) == 0:\n",
    "            full_parents.append('')\n",
    "        full_parents = list(set(full_parents))\n",
    "        \n",
    "        #print(full_parents)\n",
    "        subG = nx.subgraph(G, full_parents)\n",
    "        for c in nx.connected_components(subG):\n",
    "            parent_idx = []\n",
    "            parent_txt = []\n",
    "            for n_c in c:\n",
    "                parent_txt.append('; '.join(G.nodes[n_c]['text']))\n",
    "                parent_idx.append(n_c)\n",
    "            #parent_txt = str(parent_txt).replace('\\'', '\\\"')\n",
    "            #parent_idx = str(parent_idx).replace('\\'', '\\\"')\n",
    "            out.write('%s\\t%s\\t%s\\t%s\\n' % (n, text, json.dumps(parent_idx), json.dumps(parent_txt, ensure_ascii=False)))\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_text_component(G_full_noun, 'tt_ruthes_leaf_depth5_nouns_components_semicolon2.tsv', nodes=leaf_d5_noun)\n",
    "to_text_component(G_full_verb, 'tt_ruthes_leaf_depth5_verbs_components_semicolon2.tsv', nodes=leaf_d5_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
