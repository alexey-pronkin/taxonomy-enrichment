{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import wordpunct_tokenize\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_synset(file, all_synsets=None):\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler)\n",
    "    if all_synsets is None:\n",
    "        all_synsets = {}\n",
    "    for element in soup.findAll('synset'):\n",
    "        all_synsets[element.attrs['id']] = {'name': element.attrs['ruthes_name'], 'definition': element.attrs['definition']}\n",
    "    return all_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_senses(file):\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler)\n",
    "    all_senses = defaultdict(list)\n",
    "    for element in soup.findAll('sense'):\n",
    "        all_senses[element.attrs['synset_id']].append(element.attrs['name'])\n",
    "    return all_senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wordnet(file, synsets, senses=None, G=None, directed=False):\n",
    "    if G is None:\n",
    "        if directed:\n",
    "            G = nx.DiGraph()\n",
    "        else:\n",
    "            G = nx.Graph()\n",
    "    if directed and type(G) != nx.classes.digraph.DiGraph:\n",
    "        raise Exception('Graph is not directed')\n",
    "    if not directed and type(G) != nx.classes.digraph.Graph:\n",
    "        raise Exception('Graph should not be directed')\n",
    "    \n",
    "    print('Input graph: {} nodes, {} edges'.format(len(G.nodes), len(G.edges)))\n",
    "    handler = open(file).read()\n",
    "    soup = Soup(handler)\n",
    "    for element in soup.findAll('relation'):\n",
    "        relation = element.attrs\n",
    "        parent_id = relation['parent_id']\n",
    "        child_id = relation['child_id']\n",
    "        if relation['name'] in ['hyponym', 'instance hyponym']:\n",
    "            if parent_id not in G.nodes:\n",
    "                G.add_node(parent_id, in_edges=[], out_edges=[])\n",
    "            if child_id not in G.nodes:\n",
    "                G.add_node(child_id, in_edges=[], out_edges=[])\n",
    "            parent = G.nodes[parent_id]\n",
    "            child = G.nodes[child_id]\n",
    "            G.add_edge(parent_id, child_id)\n",
    "            if senses is not None:\n",
    "                parent_txt = copy.deepcopy(senses[parent_id])\n",
    "                child_txt = copy.deepcopy(senses[child_id])\n",
    "            else:\n",
    "                parent_txt = [synsets[parent_id]['name']]\n",
    "                child_txt = [synsets[child_id]['name']]\n",
    "            new_attr = {parent_id: {'out_edges': parent['out_edges'] + [child_id], 'text': parent_txt, 'definition': synsets[parent_id]['definition']},\n",
    "                        child_id: {'in_edges': child['in_edges'] + [parent_id], 'text': child_txt, 'definition': synsets[child_id]['definition']}}\n",
    "            nx.set_node_attributes(G, new_attr)\n",
    "    print('Updated graph: {} nodes, {} edges'.format(len(G.nodes), len(G.edges)))\n",
    "    for syn in synsets:\n",
    "        if syn not in G.nodes:\n",
    "            G.add_node(syn)\n",
    "            txt = senses[syn]\n",
    "            defn = synsets[syn]['definition']\n",
    "            nx.set_node_attributes(G, {syn: {'out_edges': [], 'in_edges': [], 'text': txt, 'definition': defn}})\n",
    "    print('Graph with orphan nodes: {} nodes, {} edges'.format(len(G.nodes), len(G.edges)))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_senses_noun = parse_senses('ruwordnet/senses.N.xml')\n",
    "all_senses_verb = parse_senses('ruwordnet/senses.V.xml')\n",
    "all_synsets_noun = parse_synset('ruwordnet/synsets.N.xml')\n",
    "all_synsets_verb = parse_synset('ruwordnet/synsets.V.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 29295 nodes, 39110 edges\n",
      "Graph with orphan nodes: 29296 nodes, 39110 edges\n",
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 7408 nodes, 10317 edges\n",
      "Graph with orphan nodes: 7521 nodes, 10317 edges\n"
     ]
    }
   ],
   "source": [
    "# wordnet graphs - undirected\n",
    "G_full_noun = parse_wordnet('ruwordnet/synset_relations.N.xml', all_synsets_noun, all_senses_noun)\n",
    "G_full_verb = parse_wordnet('ruwordnet/synset_relations.V.xml', all_synsets_verb, all_senses_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 29295 nodes, 39110 edges\n",
      "Graph with orphan nodes: 29296 nodes, 39110 edges\n",
      "Input graph: 0 nodes, 0 edges\n",
      "Updated graph: 7408 nodes, 10317 edges\n",
      "Graph with orphan nodes: 7521 nodes, 10317 edges\n"
     ]
    }
   ],
   "source": [
    "# wordnet graphs - directed\n",
    "G_full_dir_noun = parse_wordnet('ruwordnet/synset_relations.N.xml', all_synsets_noun, all_senses_noun, directed=True)\n",
    "G_full_dir_verb = parse_wordnet('ruwordnet/synset_relations.V.xml', all_synsets_verb, all_senses_verb, directed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My fitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/anton/.local/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/anton/.local/lib/python3.8/site-packages (from gensim) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/lib/python3/dist-packages (from gensim) (1.14.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107092 sha256=b2294634837d29fc94ea91bb6dc249a0d435b0d586842ece7b235df8ded52402\n",
      "  Stored in directory: /home/anton/.cache/pip/wheels/11/73/9a/f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 29296/29296 [00:00<00:00, 34793.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from node2vec.node2vec import Node2Vec\n",
    "EMBEDDING_FILENAME = './embeddings.emb'\n",
    "EMBEDDING_MODEL_FILENAME = './embeddings.model'\n",
    "\n",
    "node2vec = Node2Vec(G_full_dir_noun, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=20, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1430799 ,  0.18411247,  0.05122835,  0.16415606,  0.08352555,\n",
       "        0.08574808, -0.04463342,  0.0211307 , -0.0652193 , -0.10945312,\n",
       "        0.23427185,  0.29137558, -0.01080806,  0.07309928,  0.17517462,\n",
       "        0.10879009,  0.30468294, -0.11301188, -0.15565468, -0.15395485,\n",
       "       -0.17999066, -0.18457739,  0.20708725,  0.11832654,  0.32384712,\n",
       "       -0.14635502,  0.18172409, -0.05882047, -0.03693543, -0.15229411,\n",
       "       -0.23149192, -0.22982502,  0.01131298,  0.21117437, -0.03521312,\n",
       "        0.1346391 ,  0.25796372,  0.08345957,  0.1226878 ,  0.03634934,\n",
       "        0.20238869, -0.03589795, -0.15335676, -0.07340327, -0.19338843,\n",
       "       -0.40288144, -0.1126978 , -0.04729822, -0.07889044,  0.15825468,\n",
       "        0.01519091, -0.11495806, -0.0963335 , -0.15838763,  0.1203206 ,\n",
       "       -0.14725037, -0.00323901,  0.24607028, -0.02629884,  0.10035977,\n",
       "       -0.04297424,  0.19750354,  0.04493263,  0.01982145], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['102214-N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can get word by id from the database and do whatever we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root nodes: 9 nouns, 172 verbs\n",
      "Leaf nodes: 19083 nouns, 4631 verbs\n"
     ]
    }
   ],
   "source": [
    "roots_noun = [v for v in G_full_noun.nodes if len(G_full_noun.nodes[v]['in_edges']) == 0]\n",
    "leaves_noun = [v for v in G_full_noun.nodes if len(G_full_noun.nodes[v]['out_edges']) == 0]\n",
    "roots_verb = [v for v in G_full_verb.nodes if len(G_full_verb.nodes[v]['in_edges']) == 0]\n",
    "leaves_verb = [v for v in G_full_verb.nodes if len(G_full_verb.nodes[v]['out_edges']) == 0]\n",
    "print('Root nodes: {} nouns, {} verbs'.format(len(roots_noun), len(roots_verb)))\n",
    "print('Leaf nodes: {} nouns, {} verbs'.format(len(leaves_noun), len(leaves_verb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(G, roots, node):\n",
    "    paths = []\n",
    "    for r in roots:\n",
    "        try:\n",
    "            paths.append(nx.shortest_path_length(G, r, node))\n",
    "        except:\n",
    "            pass\n",
    "    return max(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaf nodes of depth 5+: 14649 nouns, 2357 verbs\n"
     ]
    }
   ],
   "source": [
    "leaf_d5_noun = [v for v in leaves_noun if get_depth(G_full_dir_noun, roots_noun, v) >= 5]\n",
    "leaf_d5_verb = [v for v in leaves_verb if get_depth(G_full_dir_verb, roots_verb, v) >= 5]\n",
    "print('Leaf nodes of depth 5+: {} nouns, {} verbs'.format(len(leaf_d5_noun), len(leaf_d5_verb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each connected component in its line\n",
    "# G has to be undirected\n",
    "def to_text_component(G, out_file, nodes=None, single_word=False):\n",
    "    out = open(out_file, 'w')\n",
    "    out.write('SYNSET_ID\\tTEXT\\tPARENTS\\tPARENT_TEXTS\\n')\n",
    "    if nodes is None:\n",
    "        nodes = G.nodes\n",
    "    for n in nodes:\n",
    "        \n",
    "        if single_word:\n",
    "            good_def = [txt for txt in G.nodes[n]['text'] if ' ' not in txt]\n",
    "            if len(good_def) > 0:\n",
    "                text = ','.join(good_def)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            text = '; '.join(G.nodes[n]['text'])\n",
    "        \n",
    "        parents = copy.deepcopy(G.nodes[n]['in_edges'])\n",
    "        full_parents = []\n",
    "        for p in parents:\n",
    "            p_of_p = copy.deepcopy(G.nodes[p]['in_edges'])\n",
    "            full_parents.extend([p] + p_of_p)\n",
    "        if len(full_parents) == 0:\n",
    "            full_parents.append('')\n",
    "        full_parents = list(set(full_parents))\n",
    "        \n",
    "        #print(full_parents)\n",
    "        subG = nx.subgraph(G, full_parents)\n",
    "        for c in nx.connected_components(subG):\n",
    "            parent_idx = []\n",
    "            parent_txt = []\n",
    "            for n_c in c:\n",
    "                parent_txt.append('; '.join(G.nodes[n_c]['text']))\n",
    "                parent_idx.append(n_c)\n",
    "            #parent_txt = str(parent_txt).replace('\\'', '\\\"')\n",
    "            #parent_idx = str(parent_idx).replace('\\'', '\\\"')\n",
    "            out.write('%s\\t%s\\t%s\\t%s\\n' % (n, text, json.dumps(parent_idx), json.dumps(parent_txt, ensure_ascii=False)))\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_text_component(G_full_noun, 'tt_ruthes_leaf_depth5_nouns_components_semicolon2.tsv', nodes=leaf_d5_noun)\n",
    "to_text_component(G_full_verb, 'tt_ruthes_leaf_depth5_verbs_components_semicolon2.tsv', nodes=leaf_d5_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
